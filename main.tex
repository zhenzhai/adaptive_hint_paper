%\documentclass{llncs2e/llncs}
%\usepackage{llncs2e/llncsdoc}

\documentclass{sigchi}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphics, graphicx}
\usepackage{caption}
\usepackage{csquotes}
\usepackage{balance}       % to better equalize the last page
\usepackage{graphics}      % for EPS, load graphicx instead 
\usepackage[T1]{fontenc}   % for umlauts and other diaeresis
\usepackage{txfonts}
\usepackage{mathptmx}
\usepackage{color}
\usepackage{booktabs}
\usepackage{textcomp}


% Paper metadata (use plain text, for PDF inclusion and later
% re-using, if desired).  Use \emtpyauthor when submitting for review
% so you remain anonymous.
\def\plaintitle{SIGCHI Conference Proceedings Format}
\def\plainauthor{First Author, Second Author, Third Author,
  Fourth Author, Fifth Author, Sixth Author}
\def\emptyauthor{}
\def\plainkeywords{Authors' choice; of terms; separated; by
  semicolons; include commas, within terms only; required.}
\def\plaingeneralterms{Documentation, Standardization}

% llt: Define a global style for URLs, rather that the default one
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{
    \def\UrlFont{\sf}
  }{
    \def\UrlFont{\small\bf\ttfamily}
  }}
\makeatother
\urlstyle{leo}

% To make various LaTeX processors do the right thing with page size.
\def\pprw{8.5in}
\def\pprh{11in}
\special{papersize=\pprw,\pprh}
\setlength{\paperwidth}{\pprw}
\setlength{\paperheight}{\pprh}
\setlength{\pdfpagewidth}{\pprw}
\setlength{\pdfpageheight}{\pprh}

% Make sure hyperref comes last of your loaded packages, to give it a
% fighting chance of not being over-written, since its job is to
% redefine many LaTeX commands.


% create a shortcut to typeset table headings
% \newcommand\tabhead[1]{\small\textbf{#1}}

\title{On the long term impact of Adaptive Hints}

%\author{Zhen Zhai\inst{1} \and Yoav Freund\inst{2}}
%\institute{UC San Diego \email{zzhai@eng.ucsd.edu} \and UC San Diego \email{yfreund@eng.ucsd.edu}}

\numberofauthors{2}
\author{
  \alignauthor{Zhen Zhai\\
    \affaddr{UC San Diego}\\
    \email{zzhai@eng.ucsd.edu}}\\
  \alignauthor{Yoav Freund\\
    \affaddr{UC San Diego}\\
    \email{yfreund@eng.ucsd.edu}}\\
}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

We study the impact of an adaptive hint system that we designed to help students as they are working on online assignments for a probability and statistics course.

We describe the process of constructing and sending hints. We then describe a controlled experiment we performed to test whether receiving hints improves student performance. We conclude with the support from our experimental results. Our main conclusion is that students who receive a hint for a problem in an assignment take significantly shorter time to solve problems they attempt later. We found no statistically significant change in the amount of time students take to solve the problem on which the hint was given. We also don't find a significant effect on performance in the final exam.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Introduction}

\cite{ElkherjFreund14} described a system for delivering hints in the context of a web-based homework system. In the work presented here, we describe a controlled experiment that we have done in the context of an undergraduate course in probability and statistics. Our statistical analysis provides new evidence that adaptive hints are effective.


\subsection*{Background}
The goal of Intelligent tutoring systems (ITS)\cite{Anderson1995} is to imitate a human tutor.  ITS systems detect and classify student errors and provide instantaneous feedback intended to help students
understand and correct their error. The goal is to improve
student's understanding and improve their performance on {\em future} problems, rather than ``feeding'' them the answer to the current problem.  Previous research suggests that a computer tutor can nearly be as effective as a human tutor\cite{Vanlehn2011}. Hence, many intelligent tutors have been developed and introduced into classrooms. A few well developed ITS designed for algebra curriculum in middle school and high school mathematics has already been proven
to be effective\cite{Koedinger1997, John2014}.

Research has suggested that ``Formative'' feedback is essential to improving the learning process\cite{Azevedo1995}\cite{Bangert-Drowns1991}. Formative feedback is feedback that aims to improve students learning and is presented in the form of a response to student incorrect answers\cite{Shute2008}. According to Shute~\cite{Shute2008}, formative feedback needs to be specific, clear and timely. Furthermore, formative feedback should provide learners with both verification and elaboration \cite{Mason2001} \cite{Bangert-Drowns1991}. Verification is to provide feedback to learners as to whether the answer is correct or not. Elaboration is to provide a short elaboration on the topic or discuss the learner's incorrect answer. The timing of the feedback is important as well, Kulhavy and Anderson suggest that delayed feedback is better than instance feedback\cite{Kulhavy1972}. The adaptive hint system delay hints by only providing hints to students who have spent a certain amount of time on the problem without solving it. This allows the hints only go to students who are struggling with a problem. Finally, studies show that hint-on-demand allow students to learn more as compared to proactive hints\cite{Razzaq2010}. Therefore, the adaptive hint system is designed that a student will only see a hint if they click the 'Show Hint' button.

Homework is an essential part of students' learning
process\cite{Cooper2006}. It gives students a chance to practice. Even more importantly, it allows them to identify their confusions and resolve them. There are many popular web-based homework systems for college students(e.g., WebAssign, WebWork, OWL, Andes). The goal is to help instructors to have a better management of a large enrollment class. These homework systems often provide feedback to students and provide analysis of student performance to instructors. Research has proven the web-based homework(WBH) systems to be helpful in
learning\cite{MestHartRath2002}\cite{Vanlehn2005}. In these studies, the WBH systems, OWL and Andes, provide students with immediate feedback and hints following an incorrect
attempt\cite{MestHartRath2002}\cite{Vanlehn2005}. The feedback and hints are the essential part of a WBH system, which is what our study focuses on.

\subsection*{Gaming the hint system}

There is a common concern about students gaming the system, especially
on harmful gaming, where gaming the system leads to poor learning
outcome\cite{Baker2004}\cite{Baker2005}. In previous studies, students
game the system by tricking the tutoring systems into giving out
correct answers\cite{Baker2004Off-task}. As a result, students succeed
in homework but fail on learning. Another popular type of hint is a
worked-out-examples~\cite{Atkinson2000}.  However,work by
Mclaren~\cite{McLaren2006} finds no no significant effects of
worked-ut-examples on learning in a web-based intelligent tutoring
environment

We avoid the gaming issue in by combining three design principles.
First, the number of possible answers to each question is very large
(more than 1000). Second, the information in the hint explains only
why the current answer is incorrect, it does not identify the correct
answer. Third, the hint contains a simple question designed to
highlight the mistake made by the student, so that by correctly
answering the hint the student would realize the error in their
thinking and be able to apply that knowedge to the original questin.

\iffalse
our hints don't contain correct answers in any form. The adaptive hint
system generates short simple questions as hints instead of
 Therefore, we
replaced worked examples with short questions that are designed to
promote thinking and guide student's learning process. This resolve
the concern of students gaming the system because our system gives
students hints on how to learn instead of how to answer a specific
question. The only way for a student to obtain correct answers is by
learning how to solve the problem. Therefore, we not only prevent any
system gaming, but our hints also help students in long
term. Furthermore, our hint system targets mathematic problems, which
require students to type in mathematic expressions. Therefore, unlike
multiple choice problems, it would be nearly impossible to guess the
answer by looking at hints. There is infinity many ways one can type
in a mathematic expression and it is very hard for students to simply
guess it correctly with the help of hints. Therefore, we don't have
the concern of students gaming the system.
\fi

\subsection*{Contribution}
We built and implemented an adaptive hint system to provide hints to
students at the time they are struggling with homework. The adaptive
hint system first studies a student's incorrect answer to identify the
student's confusion. We target mathematics problems that require
students to type in an expression, therefore the system generates a
message to point out the incorrect subexpression in a student's
answer. Then, based on the identified confusion, the system explains
the identified confusion with a short sentence followed by a short
question for the student to solve. This short question is specifically
designed to target the student's confusion and it is simple enough
that a student can easily understand and answer. Our goal is to let
the student solve the smaller task before moving on to solve the
bigger task. The goal of the adaptive hint system is to guides
students in their learning process from understanding their confusions
to resolve their confusions.

The system is adopted in a probability and statistic class of around 300 students and 245 students participated in this research. Each week we assign one assignment to students and we sent hints from the fourth week to the eighth week of the quarter. There are a total of 26 problems and each problem has between 1 to 10 questions. According to our record, students have received a total of 3621 hints during these five weeks. Our statistical result shows that students, with the help of the adaptive hints, learn to solve the problem and can solve similar problems more efficiently afterward. Our analysis demonstrates with statistical significant less than $1\%$ that students who receive a hint spent fewer attempts and also less amount of time on later problems in the same assignment compared to students who didn't receive a hint.


\section*{Design of the adaptive hint system}
The adaptive hint system is built on top of Open edX, an open source MOOC platform. We used Open edX as our homework system and embedded our adaptive hint system on top of Open edX. The edX homework system prompts students to answer problems by entering mathematical expressions and provides instant feedback on the correctness of the answer. The adaptive hint system provides additional feedback in the form of a hint. Students receive one assignment per week. Each assignment contains 5 to 10 problems and each problem has 1 to 10 parts. The problems in an assignment all target similar material that is covered in lecture. Students have an unlimited number of attempts for each problem, they can spend as many attempts as they want until they get to the correct answer.

Our adaptive hint system is built by creating hints beforehand based on historical data. After we release the homework, the hint system starts to parse student attempts to figure out students' confusion. Then it looks for the appropriate hint in the hint database and sends to students.


\subsection*{Creating Hints}

Hints are created before we release the homework assignments. This allows the system to have a pool of hints to start with at the time the assignment is released. To create useful hints, the teaching staff analyzes the incorrect student attempts from previous quarters and create hints accordingly. The hints created are consist of a short explanation followed by a short question for the student to answer.

For example, one question would be "How many strings contain $k$ digits and $j$ uppercase letters?". The correct answer is $10^k*26^j*C(k+j,k)$. Suppose a student answers $10^k*26^j*(k+j)!$, the system would identify the incorrect subexpression and generate the following hint

\begin{displayquote}
The subexpression $10^k*26^j$ is correct, but not $(k+j)!$. How many ways are there to arrange 2 digits and 1 letter?
\end{displayquote}


This way we encourage students to focus on the incorrect part of their answers and solve a simpler version of it before attempting to solve the original problem. Moreover, as the hint is given in the form of a question, we guide the student to think about the same problem in a simplified setting and leave it to them to infer the correct answer to the original problem. Note that students can choose to answer or not answer these hint questions. If students choose to answer the hint questions, we will provide instant feedback to let the students know whether their answers are correct or not. If students choose not to answer the hint questions, it won't hurt their grade.

Other than the problem specific hints, we also have generic hints. For example, when a student types in a number instead of an expression, our system will send a hint telling the student to type in an expression instead of the calculated result. Another example would be a hint pointing out that the question is asking for a positive number but the student typed in a negative number. These generic hints are designed to point out some very general mistakes students tend to make in a probability and statistics class. The system tends to send the generic hint before sending the specific hint.


\subsection*{Parse Trees}
The adaptive hint system needs to parse student attempts. Since we are targeting mathematical expression, we can use parse trees to parse the expressions. The parse tree always has operators as tree parents and operands as children. Each tree node is marked by its position in the tree. The root node is node $R$. The left child node is indicated by $0$ and right child is $1$. Therefore, the left tree node of node $R$ is $R.0$, and right node is $R.1$. 

To figure out the mistake in an attempt, we compare the parse tree of the attempt with the parse tree of the correct answer. The tree comparison can tell us the subexpression that students make mistakes on. We can then classify student inputs based on the incorrect subexpressions and assign hints correspondingly. For example, a student who typed $5!-2*3$ and a student who typed $\frac{5!-2*3}{10}$ for a problem with solution $\frac{5!-2*3}{3+2}$, would be classified as the same group. Both of the students will have a mismatch of tree node $R.1$. This tells the system that the subtree of $R.1$ is wrong and the system will look for the corresponding hint.

One problem is that there are many different ways to write the same mathmatical expression. For example, $5!$ can also be written as $5*4*3*2*1$ or $5*4*3*2$. Therefore, we evaluate each subtree to a numerical result and use it in our comparison. The parse trees are evaluated bottom up. The operators at the bottom of the trees are evaluated first. The root operator is evaluated last. Each operator node will therefore has a numerical result attached. And both the parse tree and the evaluated results are used when we compare the attempt to the correct answer. We identify a subtree as incorrect only when neither of the evaluation nor the subtree doesn't match. Therefore, $5!$ and $5*4*3*2*1$ will be identified as the same subtree because it evaluates to the same result. See Fig. \ref{fig:parse_tree}

\begin{figure}[ht]
  \centering
   \begin{tabular}{c c}
		\includegraphics[width=0.25\textwidth]{image/ParseTrees1.png} &
		\includegraphics[width=0.25\textwidth]{image/ParseTrees2.png}
	\end{tabular}
   \caption{A parse tree is created for every mathematical expression. Operator node is always the parent of operands, and we have the evaluated result attached on each operator node.}
   \label{fig:parse_tree}
\end{figure}

\subsection*{Assigning Hints}
When TAs create hints, they are also asked to specify rules. Hint rules tell the system when to send certain hints and who the hints will go to. One example of hint rule for expression in Fig. \ref{fig:parse_tree} could be "$R.0.0$ is wrong" and the corresponding hint could be "You need to find the number of ways to arrange different poker cards. How many ways can you arrange 2 different cards?". When the hint system captures an attempt with an incorrect $R.0.0$ subtree, this hint will be sent. Such attempts could be $\frac{4!-2*3}{3+2}$ or $\frac{30-2*3}{3+2}$. The adaptive hint system will assign hints automatically based on the hint rules. This allows our hints to be sent automatically.

Note that the system doesn't send hints to the student right away. Hints are only sent to students who have been working on the problem for more than 5 minutes or students who did more than 3 attempts. This is how the system sends delayed hint suggested by Kulhavy and Anderson\cite{Kulhavy1972}. This way hints only go to students who are working on the problem actively and are struggling. Furthermore, students need to demand hints by clicking the "Show Hints" button. Hint-on-demand is suggested to be more effective than proactive hints\cite{Razzaq2010}. Once a hint is assigned to a student, the student can choose to click or not click the "Show Hint" button. One can ignore the assigned hint by not clicking the "Show Hint" button. If a student wants to see the assigned hint, he/she needs to click the "Show Hint" button to see the hint. In other words, we only show hints to students who ask for hints. Students who don't click the "Show Hints" button will not see the assigned hints. This makes sure we give on-demand hints to students who need helps and actively ask for help. 


\section{Statistical Study of the Effectiveness of hints}

We used a controlled study to quantify the effect of our hints on
student performance. For four of the weekly assignments, we randomly
placed each student with equal odds into a case group or a control
group. We refer to those students as case students and control
students.  We use the term {\em session} to refer to the record of a
single student working on a single problem. We use the term {\em
  attempt} to refer to a single attempted answer a student sends to
the edX system.

We filter out case student sessions in which the student did not
receive a hint, either because her attempt did not trigger a hint or
because she did not click on the ``show hint'' button.  To make the
control set as similar as possible, we filter out Control student
sessions whose attempts would not have triggered a hint had they been
in the case set.

To quantify the effect we use three measures:
\begin{itemize}
\item {\bf Number of attempts:} Students were given an unlimited number of attempts. In most cases, students made attempts until they found a correct answer. We use the number of attempts to quantify the degree of difficulty experienced by the student when solving the problem.

\item {\bf Total attempt time:} In addition to the number of attempts, we estimate the total amount of time the student spent on the problem. Students periodically take a break and continue after a few hours or days, we reduce the effect of such time gaps on our estimate by eliminating any time gap longer than ten minutes.
\item {\bf Problem grade in final:} Most homework assignments have a corresonding problem in the final exam. We use the grade of said problem to quantify the long-term effect of receiving hints. 
\end{itemize}

Our goal is to establish that the hints we produce have a positive
effect on learning. The null hypothesis that we aim to reject is that
hints have no effect. We consider the potential effect at three
different time scales. The {\em within problem effect} is the effect
of receiving a hint when attempting to answer a problem on the
performance of the student {\em on the same problem}. The {\em within
  assignment effect} is the effect of receiving a hint when attempting
to solve a problem on the performance of the students on later problem
{\em within the same assignment}. Finally, the {\em long-term effect}
is the effect of receiving hints during an assignment on the
performance of the student on questions on the same material in the
final exam.

We found no statistically significant effect for either the within problem or
the long-term effect. On the other hand, we found a significant effect
on the same assignment. We present the analysis and then discuss.

\subsection{Within problem effect}

Our first attempt was to evaluate whether receiving a hint while
working on a problem helps the student solve the problem faster than
otherwise.

We consider each problem within the four weeks of our evaluation. For
each problem we compute the number of attempts and the length of time
each student spent on the problem. We compare the number of attempts
and the length of time for each problem between the case students who
received at least one hint and the control students
that would have recieved a hint were they in the case group.

The results are summarized in
Figure~\ref{fig:tries_times_analysis}. Application of a t-test fails
to detect a difference between the case and the control.

\iffalse
We also extract the time students spent on each problem. We measure the time differences between each attempt and filter time gaps that are longer than 10 minutes. Time gaps larger than 10 minutes are considered as a break instead of time spent on homework. Then we sum up all the time gaps between attempts as the total time a student spent on the problem. We again compare average time spent of case students with control students, see the graph on the right of Fig. \ref{fig:tries_times_analysis}. The graph of the number of attempts is consistent with the graph of time spent. We can't detect the effectiveness of hints on the problems where students receive and read hints and try to do the short questions in hints. Note that students who received hints are likely to spend more time on these problems because all our hints are short questions designed for students to answer. Students will likely to spend the time to read and do the hint questions.
\fi

\subsection{Within assignment effect}

Our second attempt was to evaluate whether receiving a hint while
working on an assignment helps the student solve a later problem faster
than otherwise.

We define ``problem with hint'' as follows. For the case student, a
problem with hint is a problem where a hint was sent to the student
who then chose to view it. For a control student, it is a problem for
which a hint would have been sent, had the student been in the case.

We consider only sessions that have at least one problem with
hint. For each such session, we consider all of the problems {\em
  without hint} that the student worked on after working on the
problem with hint. We call these problems the ``downstream'' problems.

For example, suppose the student solved an assignment of 9 problems in the
following order $\{ 1, 4, 3, 2, 6, 5, 7, 8, 9\}$. This student
received at least one hint on problems $\{3, 5, 7\}$. Therefore, the
downstream problems for this session are $\{2, 6, 8, 9\}$.

For each session, we compute the number of attempts (and the amount of
time) spent on each of the downstream problems. We partition these
measurements into two population depending on whether the student was
in the case or the control. See Figure \ref{fig:prob_tries_analysis} and Figure \ref{fig:prob_time_analysis}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.45\textwidth]{image/problem_tries_downstream.png}
  \caption{The graph show the analysis of the number of attempts on downstream problems. Each problem has a control set and a case set. It is shown that case students spent less number of attempts on most of the problems. The axis label also shows the p-value. For some of the problems we don't have hints sent, and therefore the data is empty.}
   \label{fig:prob_tries_analysis}
\end{figure}

\begin{figure}[ht]
  \centering
\includegraphics[width=0.45\textwidth]{image/problem_time_downstream.png}
  \caption{The graph show the analysis of the length of time spent on downstream problems. Similar to Figure \ref{fig:prob_tries_analysis}, each problem has a control set and a case set. The axis label shows the p-value. For the problems without hint sent, the data is empty. Again, case students spent less amount of time on the problems overall.}
   \label{fig:prob_time_analysis}
\end{figure}


We also did similar analysis on each assignment. We first calculate the average attempts(time) spent in control group as the baseline. Then, for each student in case, we compute the number of attempts (or the length of time) for all downstream problems and aggregate the measurement for each assignment. On the other hand, we sum the average number of attempts(or the length of time) of control group for these same downstream problems as the baseline. We then subtract the baseline from the measurement of the case student. As a result, we have a difference between the case student and the baseline. If the difference is positive, it means the case student spent more attempts(or time) on this assignment. We plot the distribution of the differences in Figure \ref{fig:downstream_tries_analysis} and Figure \ref{fig:downstream_time_analysis}. Each sub-figure represents the distribution of one assignment. The plots show that all assignment have a negative mean, meaning the case students spent less amount of tries (or time) on the downstream problems.

We then perform a one sample t-test with null hypothesis that the mean of the sum of attempts is the same as the average of attemtps. Table \ref{tab:no_hint} and Table \ref{tab:no_hint_time} show that we have small p-value for each of the assignment for both analysis of attempts and time. Therefore, we reject the null hypothesis and conclude that case students and control students have significantly different performance. This analysis shows that the adaptive hint system helps students to learn to solve similar problems that come later in the assignment. 


\begin{table}[th]
\caption{The table listed the extra number of attempts that control students spent comparing to case students. For the each of the four weeks, the control students spent more attempts than the case students. We also perform a one sample t-test with the null hypothesis that control students should have the same amount of attempts on problems as case students. The final p-value of the one sample t-test for all assignments is small enough that we can reject the null hypothesis.}
\begin{center}
\scalebox{0.6}{
  \begin{tabular}{| c | c | c | c | c | c |}
  \hline
    Week & Number& Mean of total & Std of total &  P-Value of a &Average amount \\
    Number & of & attempts & attempts & two sided one & of total attempt spent\\
     & instances & differences & differences & sample t-test & on assignment   \\ \hline
	4 & 43 & -22.234 & 26.825 & $3.16 * 10^{-6}$ & \\
	5 & 81 & -43.128 & 35.508 & $2.058 * 10 ^{-17}$ & \\
	6 & 64 & -19.955 & 29.943 & $1.641 * 10^{-6}$ &  \\
	7 & 55 & -86.785 & 39.392 & $2.250 * 10^{-22}$ & \\
	\hline
  \end{tabular}}
  \label{tab:no_hint}
  \end{center}
\end{table}

\begin{table}
\caption{The table listed the extra minutes that control spent comparing to case. We first compare the time spent for each assignment, then we look at all assignments at the end. We perform a two tailed t-test with the null hypothesis that control students spent the same amount of time on problems as case students. The final p-value of the two tailed t-test for all assignments is small enough that we can reject the null hypothesis.}
\begin{center}
\scalebox{0.6}{
  \begin{tabular}{| c | c | c | c | c | c |}
    \hline
    Week & Number& Mean of total & Std of total &  P-Value of a &Average amount \\
    Number & of & time(seconds) & time(seconds) & two sided one & of total time spent\\
     & instances & differences & differences & sample t-test & on assignment   \\ \hline
	4 & 41 & -833.087 & 2983.083 & 0.085 & \\
	5 & 62 & -1989.48 &1887.724 & $1.761 * 10^{-11}$ & \\
	6 & 58 & -1396.852 & 3850.474 & 0.0082 &  \\
	7 & 53 & -5438.16 & 5671.409 & $6.77 * 10^{-9}$ & \\
	\hline
  \end{tabular}}
  \label{tab:no_hint_time}
  \end{center}
\end{table}


\begin{figure}[ht]
\includegraphics[width=0.5\textwidth]{image/assignment_tries_downstream.png}
\caption{The graphs show the distribution of the extra attempts that case student spent comparing to control students. Each subplot corresponds to the distribution of an assignment. The graphs show that the control students spent more attempts on each assignment comparing to the case students.}
    \label{fig:downstream_tries_analysis}
\end{figure}

\begin{figure}[ht]
\includegraphics[width=0.5\textwidth]{image/assignment_time_downstream.png}
\caption{The graphs show the distribution of extra time (in seconds) that case students spent comparing to control students. All distribution have a negative mean. Control students again spend more time on each assignment than the case students. Each subplot corresponds to one assignment. The results are consistent with Figure \ref{fig:downstream_tries_analysis}}
    \label{fig:downstream_time_analysis}
\end{figure}

\subsection{Long term effect}
Final scores play an important role as to evaluate the learning outcome of students. We examine whether hints have effect on students' final score.

The problems in the final exam aim to examine students understanding of different topics learned throughout the college quarter. Each of these topics have a collection of homework problems for students to practice. Therefore, we select a set of homework problems for each of the final problem as shown in Table \ref{tab:map}.

\begin{table}[h]
\caption{Each final problem on the left has a set of corresponding homework problems on the right. The problem ID is in the format of week number and problem number. For example, (4,1) means problem 1 of week 4 assignment.}
\begin{center}
  \begin{tabular}{ c | c }
   Final Problem & Homework Problem IDs \\ \hline
	3 & (4,1), (4,2) \\
	4 & (5,1), (5,2), (5,3), (5,4) \\
    5 & (6,4) \\
    6 & (6,6) \\
    7 & (6,2), (6,3) \\
    8 & (8,3), (8,4), (8,5), (8,6) \\ \hline
  \end{tabular}
  \label{tab:map}
  \end{center}
\end{table}

For each final problem, we look at the corresponding assignment. The control and case students of the corresponding assignment will then be the control and case students of the final problem. We compare control and case students on their final score for each final problem. See Figure \ref{fig:final_compare_all}.

\iffalse
\begin{figure}[h]
\centering
\caption{The graph shows average final scores of each final problem. The data points are labeled with p-value of a two-tailed t-test. We don't see a very clear advantage on either of the control or case.}
\includegraphics[width=0.8\linewidth, height=5cm]{image/final_compare.png}
\label{fig:final_compare_all}
\end{figure}
\fi

We can't tell which group of students perform better than the other. The p-values are not small enough and we conclude that the effect of hints on final scores is non-detectable.


\section{Conclusion}
We built an adaptive hint system for mathematics problems that can be embedded on any web-based homework platform. The system provides students with on-demand formative feedback to guide their learning process to achieve better learning outcome. Although we can't detect an improved in the final exam, our statistics show an improvement of students' performance on homework. We show that students with the help of the adaptive hints spent less time and less attempts on homework problems compared to students without the help of hints.

\balance{}

%\bibliographystyle{splncs}
\bibliographystyle{SIGCHI-Reference-Format}
\bibliography{bibtxt}



\end{document}
