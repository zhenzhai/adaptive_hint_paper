%\documentclass{llncs2e/llncs}
%\usepackage{llncs2e/llncsdoc}

\documentclass{sigchi/sigchi}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphics, graphicx}
\usepackage{caption}
\usepackage{csquotes}


\title{On the impact of Adaptive Hints}

%\author{Zhen Zhai\inst{1} \and Yoav Freund\inst{2}}
%\institute{UC San Diego \email{zzhai@eng.ucsd.edu} \and UC San Diego \email{yfreund@eng.ucsd.edu}}

\numberofauthors{2}
\author{
  \alignauthor{Zhen Zhai\\
    \affaddr{UC San Diego}\\
    \email{zzhai@eng.ucsd.edu}}\\
  \alignauthor{Yoav Freund\\
    \affaddr{UC San Diego}\\
    \email{yfreund@eng.ucsd.edu}}\\
}

\begin{document}

\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
We study the impact of an adaptive hint system we designed to help students as they are working on online assignments for a probability and statistics course.

We describe the process of constructing and sending hints. We then describe a controlled experiment we performed to test whether receiving hints improves student performance. We conclude with the experimental results. Our main conclusion is that students the receive a hint for a problem in an assignment take significantly shorter time to solve problems they attempt later. We found no statistically significant change in the amount of time students take to solve the problem on which the hint was given. We also don't find a significant effect on performance in the final exam.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse
\section*{Introduction}

\cite{ElkherjFreund14} described a system for delivering hints in the
context of a web-based homework system. In the work presented here, we
describe a controlled experiment that we have done in the context of
an undergraduate course in probability and statistics. Our statical
analysis provides new evidence that adaptive hints are effective.
\fi

\section*{Background}
The goal of Intelligent tutoring systems (ITS)\cite{Anderson1995} is
to immitate a human tutor.  ITS systems detect and classify student
errors and provide instantaneous feedback intended to help students
understand and correct correct their error. The goal is to improve
student's understanding and improve their performance on {\em future}
problems, rather than ``feeding'' them the answer to the current
pproblem.  Previous research suggests that a computer tutor can nearly
be as effective as a human tutor\cite{Vanlehn2011}. Hence, many
intelligent tutors have been developed and introduced into
classrooms. A few well developed ITS designed for algebra curriculum
in middle school and high school mathematics has already been proven
to be effective\cite{Koedinger1997,John2014}.

Research has suggested that ``Formative'' feedback is essential to
improving the learning
process\cite{Azevedo1995}\cite{Bangert-Drowns1991}. Formative feedback
is feedback that aims to improve students learning and is presented in
the form of a response to student incorrect
answers\cite{Shute2008}. According to Shute~\cite{Shute2008},
formative feedback needs to be specific, clear and
timely. Furthermore, formative feedback should provide learners with
both verification and
elaboration\cite{Mason2001}\cite{Bangert-Drowns1991}. Verification is
to provide feedback to learners as to whether the answer is correct or
not. Elaboration is to provide a short elaboration on the topic or
discuss the learner's incorrect answer. The timing of the feedback is
important as well, Kulhavy and Anderson suggest that delayed feedback
is better than instance feedback\cite{Kulhavy1972}. The adaptive hint
system delay hints by only providing hints to students who have spent
a certain amount time on the problem without solving it. This allows
the hints only go to students who are struggling with a
problem. Finally, studies show that hint-on-demand allow students to
learn more as compared to proactive hints\cite{Razzaq2010}. Therefore,
the adaptive hint system is designed that a student will only see a
hint if they click the 'Show Hint' button.

Homework is an essential part of students' learning
process\cite{Cooper2006}. It gives students a chance to practice. Even
more importantly, it allows them to identify their confusions and
resolve them. There are many popular web-based homework systems for
college students(e.g., WebAssign, WebWork, OWL, Andes). The goal is to
help instructors to have a better management of a large enrollment
class. These homework systems often provide feedback to students and
provide analysis of student performance to instructors. Research has
proven the web-based homework(WBH) systems to be helpful in
learning\cite{MestHartRath2002}\cite{Vanlehn2005}. In these studies,
the WBH systems, OWL and Andes, provide students with immediate
feedback and hints following an incorrect
attempt\cite{MestHartRath2002}\cite{Vanlehn2005}. The feedback and
hints are the essential part of a WBH system, which is what our study
focuses on.

\subsection*{Gaming the hint system}

A common concern regarding hint systems is the ability of students to
game the system and extract the correct answer without understanding
it.  \cite{Baker2004}\cite{Baker2005}. In previous studies, students
have been shown to game the system by manipulating the tutoring system
into giving out correct answers\cite{Baker2004Off-task}. As a result,
students succeed in homework but fail on learning. Another popular
type of hint is a worked-out-examples~\cite{Atkinson2000}.
However,work by Mclaren~\cite{McLaren2006} finds no no significant
effects of worked-ut-examples on learning in a web-based intelligent
tutoring environment

We avoid the gaming issue in by combining three design principles.
First, the number of possible answers to each question is very large
(typicalle more than 1000) making guessing the correct answer
imprtactical.  Second, the information contained in the hint
identifies only the reason that the current answer is incorrect, it
does not identify the correct answer. Third, the hint contains a
simple question designed to highlight the mistake made by the student,
so that by correctly answering the hint the student would realize the
error in their thinking and be able to apply that knowedge to the
original questin.

\iffalse
our hints don't contain correct answers in any form. The adaptive hint
system generates short simple questions as hints instead of
 Therefore, we
replaced worked examples with short questions that are designed to
promote thinking and guide student's learning process. This resolve
the concern of students gaming the system because our system gives
students hints on how to learn instead of how to answer a specific
question. The only way for a student to obtain correct answers is by
learning how to solve the problem. Therefore, we not only prevent any
system gaming, but our hints also help students in long
term. Furthermore, our hint system targets mathematic problems, which
require students to type in mathematic expressions. Therefore, unlike
multiple choice problems, it would be nearly impossible to guess the
answer by looking at hints. There is infinity many ways one can type
in a mathematic expression and it is very hard for students to simply
guess it correctly with the help of hints. Therefore, we don't have
the concern of students gaming the system.
\fi

\subsection*{Contribution}
We extended and deployed the adaptive hint system described
in~\cite{ElkherjFreund14}. The deployment required the carefull design
of 162 hints by the course staff consisting of an instructor, four
TAs, and four tutors. The hints were designed based on logs from a previous
deployment. As part of the deployment we carried out a controlled
experiment to measure the effect of the hints.

The system wa deployed in a probability and statistic class of around
300 students and 245 students participated in this research. The hint
system was used for 4 out of the eight assignments in the course
(other assignment were either too simple or did not lend themselves to
our methodology).  There are a total of 26 problems and each problem
has between 1 to 10 questions each requiring student response. In
total, tudents have received a 3621 hints during these four weeks.

The main contribution of this paper is a demonstration, with high
statistical significance, that hints improve students
learning. Specifically, we should that students that recieve a hint at
a point of time are likely to perform better on problems attempted
later. On the other hand, we could detect no significant effect on
performance on the problem for which the hint was given (immediate
effect) nor on the performance of the student in the final exam.

\section*{Design of the adaptive hint system}
The adaptive hint system is built on top of Open edX, an open source
MOOC platform. We used Open edX as our homework system and embedded
our adaptive hint system on top of Open edX. The edX homework system
prompts students to answer problems by entering mathematical
expressions and provides instant feedback on the correctness of the
answer. The adaptive hint system provides additional feedback in the
form of a hint. Students receive one assignment per week. Each
assignment contains 5 to 10 problems and each problem has 1 to 10
parts. The problems in an assignment all target similar material that
is covered in lecture. Students have an unlimited number of attempts
for each problem, they can spend as many attempts as they want until
they get to the correct answer.

Our adaptive hint system is based on a database of hints populated
manually before each assignment is released, based
on historical data~\footnote{It is possible to add hints in
real-time, however, doing so in practice proved hard.}. Each hint
consists of the following three parts:
\begin{enumerate}
\item {\bf Trigger:} a condition on the student's attempt that will
  cause the hint to be sent.
\item {\bf Message:} A message to the student identifying what is
  wrong in the attempt.
\item {\bf Question:} A simple question designed to help the student
  understand their mistake. Together with the correct answer to the
  question.}    
\end{enumerate}

We want to make sure that students try to solve the problem on their
own before presenting them with a hint. The system therefor delays
sending the hint until the student spent on it XX minutes and
made at least YY attempts.

Some students are not interestedin hints and will not read them even
if they are presented to them. In order to identify whether the
student is interested in reading the hint the system first displays on
the web page a button saying ``Show Hint''. The hint is shown to the
student only after clicking on the button, thereby showing interest in
the hint. We use this button-press as a proxy for ``student read the hint''..

Students can choose to whether or not to answer the hint question. If
students choose to answer the hint question, the system provides
instant feedback to let the students know whether their answers are
correct or not. We make it clear to the students that hints will not
affect their grade. In particular, they do not have to answer hint
question.

\subsection*{Hints}

We support two types of hints, generic and specific. Generic hints
point out a basic problem with the student's answer, while specific
hints point a particular common type of error for a particular part of a
particular problem, specific hints are usually based on finding an
error in a mathematical expression using parsing, as will be explained below.

Some of the generic hints point our simple errors such as ``The size
of a set cannot be negative'' or ``A probability cannot be large than
1''.   Another generic hint is sent when a student gives an incorrect answer,
and that answer is a number, rather than an expression. This exposes
more of the student's thinking process and allows specific hints to be sent.

Specific hints are based on the parsing of a mathematical expression,
we explain parsing expression in the next section.

As an example of specific hint consider the problem ``How many strings
contain $k$ digits and $j$ uppercase letters?''. The correct answer is
$10^k*26^j*C(k+j,k)$. Suppose a student answers $10^k*26^j*(k+j)!$,
the system would identify the incorrect subexpression and generate the
following hint

\begin{displayquote}
The subexpression $10^k*26^j$ is correct, but not $(k+j)!$. How many
ways are there to arrange 2 digits and 1 letter? 
\end{displayquote}

This specific hint identifies the correct part of the answer and
directs the studen to focus on the incorrect part of their answers and
solve a simpler version of it before attempting to solve the original
problem. Moreover, as the hint is given in the form of a question, we
guide the student to think about the same problem in a simplified
setting and leave it to them to infer the correct answer to the
original problem. 

Creating good specific hints is labor intensive: the instructor needs
to identify a common type of mistake find a trigger to detect that
mistake, and write a hint and a question that will nudge the student
in the right direction. However, as we show in the analysis, this
effort pays off in imprved learning.

\subsection*{Triggers and Parsing}

A typical problem contains several small constants


The adaptive hint system needs to parse student attempts. Since we are
targeting mathematical expression, we can use parse trees to parse the
expressions. The parse tree always has operators as tree parents and
operands as children. Each tree node is marked by its position in the
tree. The root node is node $R$. The left child node is indicated by
$0$ and right child is $1$. Therefore, the left tree node of node $R$
is $R.0$, and right node is $R.1$.

To figure out the mistake in an attempt, we compare the parse tree of
the attempt with the parse tree of the correct answer. The tree
comparison can tell us the subexpression that students make mistakes
on. We can then classify student inputs based on the incorrect
subexpressions and assign hints correspondingly. For example, a
student who typed $5!-2*3$ and a student who typed $\frac{5!-2*3}{10}$
for a problem with solution $\frac{5!-2*3}{3+2}$, would be classified
as the same group. Both of the students will have a mismatch of tree
node $R.1$. This tells the system that the subtree of $R.1$ is wrong
and the system will look for the corresponding hint.

One problem is that there are many different ways to write the same
mathmatical expression. For example, $5!$ can also be written as
$5*4*3*2*1$ or $5*4*3*2$. Therefore, we evaluate each subtree to a
numerical result and use it in our comparison. The parse trees are
evaluated bottom up. The operators at the bottom of the trees are
evaluated first. The root operator is evaluated last. Each operator
node will therefore has a numerical result attached. And both the
parse tree and the evaluated results are used when we compare the
attempt to the correct answer. We identify a subtree as incorrect only
when neither of the evaluation nor the subtree doesn't
match. Therefore, $5!$ and $5*4*3*2*1$ will be identified as the same
subtree because it evaluates to the same result. See
Fig. \ref{fig:parse_tree}

\begin{figure}[ht]
  \centering
   \begin{tabular}{c c}
		\includegraphics[width=0.25\textwidth]{image/ParseTrees1.png} &
		\includegraphics[width=0.25\textwidth]{image/ParseTrees2.png}
	\end{tabular}
   \caption{A parse tree is created for every mathematical expression. Operator node is always the parent of operands, and we have the evaluated result attached on each operator node.}
   \label{fig:parse_tree}
\end{figure}

\subsection*{Assigning Hints}
When TAs create hints, they are also asked to specify rules. Hint rules tell the system when to send certain hints and who the hints will go to. One example of hint rule for expression in Fig. \ref{fig:parse_tree} could be "$R.0.0$ is wrong" and the corresponding hint could be "You need to find the number of ways to arrange different poker cards. How many ways can you arrange 2 different cards?". When the hint system captures an attempt with an incorrect $R.0.0$ subtree, this hint will be sent. Such attempts could be $\frac{4!-2*3}{3+2}$ or $\frac{30-2*3}{3+2}$. The adaptive hint system will assign hints automatically based on the hint rules. This allows our hints to be sent automatically.

Note that the system doesn't send hints to the student right away. Hints are only sent to students who have been working on the problem for more than 5 minutes or students who did more than 3 attempts. This is how the system sends delayed hint suggested by Kulhavy and Anderson\cite{Kulhavy1972}. This way hints only go to students who are working on the problem actively and are struggling. Furthermore, students need to demand hints by clicking the "Show Hints" button. Hint-on-demand is suggested to be more effective than proactive hints\cite{Razzaq2010}. Once a hint is assigned to a student, the student can choose to click or not click the "Show Hint" button. One can ignore the assigned hint by not clicking the "Show Hint" button. If a student wants to see the assigned hint, he/she needs to click the "Show Hint" button to see the hint. In other words, we only show hints to students who ask for hints. Students who don't click the "Show Hints" button will not see the assigned hints. This makes sure we give on-demand hints to students who need helps and actively ask for help. 


\section{Statistical Study of the Effectiveness of hints}

We used a controlled study to quantify the effect of our hints on
student performance. For four of the weekly assignments, we randomly
placed each student with equal odds into a case group or a control
group. We refer to those students as case students and control
students.  We use the term {\em session} to refer to the record of a
single student working on a single problem. We use the term {\em
  attempt} to refer to a single attempted answer a student sends to
the edX system.

We filter out case student sessions in which the student did not
receive a hint, either because her attempt did not trigger a hint or
because she did not click on the ``show hint'' button.  To make the
control set as similar as possible, we filter out Control student
sessions whose attempts would not have triggered a hint had they been
in the case set.

To quantify the effect we use three measures:
\begin{itemize}
\item {\bf Number of attempts:} Students were given an unlimited number of attempts. In most cases, students made attempts until they found a correct answer. We use the number of attempts to quantify the degree of difficulty experienced by the student when solving the problem.

\item {\bf Total attempt time:} In addition to the number of attempts, we estimate the total amount of time the student spent on the problem. Students periodically take a break and continue after a few hours or days, we reduce the effect of such time gaps on our estimate by eliminating any time gap longer than ten minutes.
\item {\bf Problem grade in final:} Most homework assignments have a corresonding problem in the final exam. We use the grade of said problem to quantify the long-term effect of receiving hints. 
\end{itemize}

Our goal is to establish that the hints we produce have a positive
effect on learning. The null hypothesis that we aim to reject is that
hints have no effect. We consider the potential effect at three
different time scales. The {\em within problem effect} is the effect
of receiving a hint when attempting to answer a problem on the
performance of the student {\em on the same problem}. The {\em within
  assignment effect} is the effect of receiving a hint when attempting
to solve a problem on the performance of the students on later problem
{\em within the same assignment}. Finally, the {\em long-term effect}
is the effect of receiving hints during an assignment on the
performance of the student on questions on the same material in the
final exam.

We found no statistically significant effect for either the within problem or
the long-term effect. On the other hand, we found a significant effect
on the same assignment. We present the analysis and then discuss.

\subsection{Within problem effect}

Our first attempt was to evaluate whether receiving a hint while
working on a problem helps the student solve the problem faster than
otherwise.

We consider each problem within the four weeks of our evaluation. For
each problem we compute the number of attempts and the length of time
each student spent on the problem. We compare the number of attempts
and the length of time for each problem between the case students who
received at least one hint and the control students
that would have recieved a hint were they in the case group.

The results are summarized in
Figure~\ref{fig:tries_times_analysis}. Application of a t-test fails
to detect a difference between the case and the control.

\iffalse
We also extract the time students spent on each problem. We measure the time differences between each attempt and filter time gaps that are longer than 10 minutes. Time gaps larger than 10 minutes are considered as a break instead of time spent on homework. Then we sum up all the time gaps between attempts as the total time a student spent on the problem. We again compare average time spent of case students with control students, see the graph on the right of Fig. \ref{fig:tries_times_analysis}. The graph of the number of attempts is consistent with the graph of time spent. We can't detect the effectiveness of hints on the problems where students receive and read hints and try to do the short questions in hints. Note that students who received hints are likely to spend more time on these problems because all our hints are short questions designed for students to answer. Students will likely to spend the time to read and do the hint questions.
\fi

\subsection{Within assignment effect}

Our second attempt was to evaluate whether receiving a hint while
working on an assignment helps the student solve a later problem faster
than otherwise.

We define ``problem with hint'' as follows. For the case student, a
problem with hint is a problem where a hint was sent to the student
who then chose to view it. For a control student, it is a problem for
which a hint would have been sent, had the student been in the case.

We consider only sessions that have at least one problem with
hint. For each such session, we consider all of the problems {\em
  without hint} that the student worked on after working on the
problem with hint. We call these problems the ``downstream'' problems.

For example, suppose the student solved an assignment of 9 problems in the
following order $\{ 1, 4, 3, 2, 6, 5, 7, 8, 9\}$. This student
received at least one hint on problems $\{3, 5, 7\}$. Therefore, the
downstream problems for this session are $\{2, 6, 8, 9\}$.

For each session, we compute the number of attempts (and the amount of
time) spent on each of the downstream problems. We partition these
measurements into two population depending on whether the student was
in the case or the control. See Figure \ref{fig:prob_analysis}.

\begin{figure}[ht]
  \centering
   \begin{tabular}{c c}
		\includegraphics[width=0.25\textwidth]{image/problem_tries_downstream.png} &
		\includegraphics[width=0.25\textwidth]{image/problem_time_downstream.png}
	\end{tabular}
   \caption{Both graphs show the analysis of downstream problems. The graph on the left shows the number of attempts for each problem and the graph on the right shows the length of time spent on each problem.}
   \label{fig:prob_analysis}
\end{figure}

We also did similar analysis on each assignment. We first calculate the average attempts(time) spent in control group as the baseline. Then, for each student in case, we compute the number of attempts (or the length of time) for all downstream problems and aggregate the measurement for each assignment. On the other hand, we sum the average number of attempts(or the length of time) of control group for these same downstream problems as the baseline. We then subtract the baseline from the measurement of the case student. As a result, we have a difference between the case student and the baseline. If the difference is positive, it means the case student spent more attempts(or time) on this assignment. We plot the distribution of the differences in Figure \ref{fig:downstream_tries_analysis} and Figure \ref{fig:downstream_time_analysis}. Each sub-figure represents the distribution of one assignment. The plots show that all assignment have a negative mean, meaning the case students spent less amount of tries (or time) on the downstream problems.

We then perform a one sample t-test with null hypothesis that the mean of the sum of attempts is the same as the average of attemtps. Table \ref{tab:no_hint} and Table \ref{tab:no_hint_time} show that we have small p-value for each of the assignment for both analysis of attempts and time. Therefore, we reject the null hypothesis and conclude that case students and control students have significantly different performance. This analysis shows that the adaptive hint system helps students to learn to solve similar problems that come later in the assignment. 


\begin{table}[th]
\caption{The table listed the extra number of attempts that control students spent comparing to case students. For the each of the four weeks, the control students spent more attempts than the case students. We also perform a one sample t-test with the null hypothesis that control students should have the same amount of attempts on problems as case students. The final p-value of the one sample t-test for all assignments is small enough that we can reject the null hypothesis.}
\begin{center}
\scalebox{0.6}{
  \begin{tabular}{| c | c | c | c | c | c |}
  \hline
    Week & Number& Mean of total & Std of total &  P-Value of a &Average amount \\
    Number & of & attempts & attempts & two sided one & of total attempt spent\\
     & instances & differences & differences & sample t-test & on assignment   \\ \hline
	4 & 43 & -22.234 & 26.825 & $3.16 * 10^{-6}$ & \\
	5 & 81 & -43.128 & 35.508 & $2.058 * 10 ^{-17}$ & \\
	6 & 64 & -19.955 & 29.943 & $1.641 * 10^{-6}$ &  \\
	7 & 55 & -86.785 & 39.392 & $2.250 * 10^{-22}$ & \\
	\hline
  \end{tabular}}
  \label{tab:no_hint}
  \end{center}
\end{table}

\begin{table}
\caption{The table listed the extra minutes that control spent comparing to case. We first compare the time spent for each assignment, then we look at all assignments at the end. We perform a two tailed t-test with the null hypothesis that control students spent the same amount of time on problems as case students. The final p-value of the two tailed t-test for all assignments is small enough that we can reject the null hypothesis.}
\begin{center}
\scalebox{0.6}{
  \begin{tabular}{| c | c | c | c | c | c |}
    \hline
    Week & Number& Mean of total & Std of total &  P-Value of a &Average amount \\
    Number & of & time(seconds) & time(seconds) & two sided one & of total time spent\\
     & instances & differences & differences & sample t-test & on assignment   \\ \hline
	4 & 41 & -833.087 & 2983.083 & 0.085 & \\
	5 & 62 & -1989.48 &1887.724 & $1.761 * 10^{-11}$ & \\
	6 & 58 & -1396.852 & 3850.474 & 0.0082 &  \\
	7 & 53 & -5438.16 & 5671.409 & $6.77 * 10^{-9}$ & \\
	\hline
  \end{tabular}}
  \label{tab:no_hint_time}
  \end{center}
\end{table}


\begin{figure}[ht]
\includegraphics[width=0.5\textwidth]{image/assignment_tries_downstream.png}
\caption{The graphs show the distribution of the extra attempts that case student spent comparing to control students. Each subplot corresponds to the distribution of an assignment. The graphs show that the control students spent more attempts on each assignment comparing to the case students.}
    \label{fig:downstream_tries_analysis}
\end{figure}

\begin{figure}[ht]
\includegraphics[width=0.5\textwidth]{image/assignment_time_downstream.png}
\caption{The graphs show the distribution of extra time (in seconds) that case students spent comparing to control students. All distribution have a negative mean. Control students again spend more time on each assignment than the case students. Each subplot corresponds to one assignment. The results are consistent with Figure \ref{fig:downstream_tries_analysis}}
    \label{fig:downstream_time_analysis}
\end{figure}

\subsection{Long term effect}
Final scores play an important role as to evaluate the learning outcome of students. We examine whether hints have effect on students' final score.

The problems in the final exam aim to examine students understanding of different topics learned throughout the college quarter. Each of these topics have a collection of homework problems for students to practice. Therefore, we select a set of homework problems for each of the final problem as shown in Table \ref{tab:map}.

\begin{table}[h]
\caption{Each final problem on the left has a set of corresponding homework problems on the right. The problem ID is in the format of week number and problem number. For example, (4,1) means problem 1 of week 4 assignment.}
\begin{center}
  \begin{tabular}{ c | c }
   Final Problem & Homework Problem IDs \\ \hline
	3 & (4,1), (4,2) \\
	4 & (5,1), (5,2), (5,3), (5,4) \\
    5 & (6,4) \\
    6 & (6,6) \\
    7 & (6,2), (6,3) \\
    8 & (8,3), (8,4), (8,5), (8,6) \\ \hline
  \end{tabular}
  \label{tab:map}
  \end{center}
\end{table}

For each final problem, we look at the corresponding assignment. The control and case students of the corresponding assignment will then be the control and case students of the final problem. We compare control and case students on their final score for each final problem. See Figure \ref{fig:final_compare_all}.

\iffalse
\begin{figure}[h]
\centering
\caption{The graph shows average final scores of each final problem. The data points are labeled with p-value of a two-tailed t-test. We don't see a very clear advantage on either of the control or case.}
\includegraphics[width=0.8\linewidth, height=5cm]{image/final_compare.png}
\label{fig:final_compare_all}
\end{figure}
\fi

We can't tell which group of students perform better than the other. The p-values are not small enough and we conclude that the effect of hints on final scores is non-detectable.


\section{Conclusion}
We built an adaptive hint system for mathematics problems that can be embedded on any web-based homework platform. The system provides students with on-demand formative feedback to guide their learning process to achieve better learning outcome. Although we can't detect an improved in the final exam, our statistics show an improvement of students' performance on homework. We show that students with the help of the adaptive hints spent less time and less attempts on homework problems compared to students without the help of hints.

\newpage
\bibliographystyle{splncs}
\bibliography{bibtxt}



\end{document}
